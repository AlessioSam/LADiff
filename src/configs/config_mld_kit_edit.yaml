####################################
# The following are general settings
####################################

# Experiment name, more details in Section 'Experiment Name Explanation'
NAME: KIT_LADiff_48latent_s1-11199-33D
# Debug mode. Set to True will enter the debug mode, then the program will
# 1. use a tiny dataset for trianing and evaluation
# 2. validate more intensively
# 3. will not use `wandb logger`
DEBUG: False
# Devices. Optional: “cpu”, “gpu”
ACCELERATOR: 'gpu'
# Index of GPUs eg. [0] or [0,1,2,3]
DEVICE: [0,1,2,3,4,5,6,7] #[3,5,6,7] #[1,4,5,6] #[0,2,3,4] #[0,1,2,3] # [3,4,5,6] nostre
TEST_DEVICE: [0]

IDEA: 'ard'
ARDIFF: False
FORCE_SEQ: False

#####################################
# The following are training settings
#####################################
TRAIN:
  # Model stage. Optional: "vae", "diffusion"
  STAGE: diffusion # diffusion
  # Training dataset name
  DATASETS: ['kit']
  # DATASETS CONFIG FOR FORCING SEQUENTIALITY
  SUBPHASE: None #stage2 #'stage2' # stage1, stage2, None
  N_FRAMES: None #8
  # Number of dataloader workers
  NUM_WORKERS: 8
  # Size of batches
  BATCH_SIZE: 128
  # Total epochs for training
  END_EPOCH: 25000
  PRETRAINED: '' #'./experiments/mld/ar_diff_v4_AdaDVAE_16latent_s1-2999-50D_BothDecoder/checkpoints/epoch=4399.ckpt' #'./experiments/mld/DVAE_16latent_50D/checkpoints/epoch=1199.ckpt' #'./experiments/mld/DVAE_16latent_50D_continued_OK/checkpoints/epoch=1799.ckpt' # vae model path


  RESUME_STAGE1: '' #'./experiments/mld/vae_7latent_DVAE/checkpoints/epoch=2799.ckpt' #'./experiments/mld/vae_7latent_mld_original/checkpoints/epoch=5999.ckpt' #'./experiments/mld/vae_7latent_mld_original/checkpoints/epoch=2799.ckpt' # was 2799

  RESUME: '' #'./experiments/mld/ar_diff_v4_AdaDVAE_16latent_s1-1199-50D_BothDecoder' #'./experiments/mld/ar_diff_v4_AdaDVAE_16latent_s1-1199_BothDecoder' #'./experiments/mld/ar_diff_v1-1_s1finish' # Resume training from this path
  PRETRAINED_VAE: './experiments/mld/KIT_DVAE_48latent_33D_reweight_from3199/checkpoints/epoch=8199.ckpt' # vae model path
  OPTIM:
    TYPE: AdamW # Optimizer type
    LR: 1e-4 # Learning rate
  # Ablation study configurations.
  ABLATION:
    SKIP_CONNECT: True
    PE_TYPE: mld
    DIFF_PE_TYPE: mld
    IDEA: 'ard'
    DVAE: False
    PERCENTAGE_NOISED: 0.0
    FINETUNE_DECODER: False
    MAX_IT: 5 #7 #13 
    FRAME_PER_LATENT: 48 #32 #16
    MD_TRANS: True
    PE_LATENT: False
    JOINT_DISTRO_FIX: False
    LAD: True

#####################################
# The following are validation settings
#####################################
EVAL:
  DATASETS: ['kit'] # Evaluating datasets
  BATCH_SIZE: 32 # Evaluating Batch size
  SPLIT: test

#####################################
# The following are testing settings
#####################################
TEST:           
  CHECKPOINTS: './experiments/mld/KIT_LADiff_48latent_s1-11199-33D/checkpoints/epoch=1999.ckpt'
  RESUME_STAGE1: '' #'./experiments/mld/vae_7latent_DVAE/checkpoints/epoch=1199.ckpt'
  DATASETS: ['kit'] # training datasets
  SPLIT: test
  BATCH_SIZE: 128 #64 # training Batch size
  MEAN: False
  NUM_SAMPLES: 1
  FACT: 1
  REPLICATION_TIMES: 20 # Number of times to replicate the test
  SAVE_LATENTS: False
  

#####################################
# The following are basic datasets settings
#####################################
DATASET:
  JOINT_TYPE: 'humanml3d' # join type
  LOAD_LATENTS: None

#####################################
# The following are metric settings
#####################################
METRIC:
  TYPE: ['TemosMetric', 'TM2TMetrics']

#####################################
# The following are training losses settings
#####################################
LOSS:
  TYPE: mld # Losses type
  LAMBDA_LATENT: 1.0e-5 # Lambda for latent Losses
  LAMBDA_KL: 1.0e-4 # Lambda for kl Losses
  LAMBDA_REC: 1.0 # Lambda for reconstruction Losses
  LAMBDA_JOINT: 0.005 # Lambda for joint Losses
  LAMBDA_GEN: 1.0 # Lambda for text-motion generation losses
  LAMBDA_CROSS: 1.0 # Lambda for reconstruction Losses
  LAMBDA_CYCLE: 0.0 # Lambda for cycle Losses
  LAMBDA_PRIOR: 0.0
  DIST_SYNC_ON_STEP: False # Sync Losses on step when distributed trained

#####################################
# The following are basic model settings
#####################################
model:
  vae: true # whether vae model
  model_type: mld # model type
  condition: 'text'
  ff_size: 1024 #!! 1024 #
  num_layers: 9 # number of layers
  num_head: 4 # number of head layers
  droupout: 0.1 # dropout rate
  activation: gelu # activation type
  guidance_scale: 7.5 #
  guidance_uncondp: 0.1 # 0.1 0.25
  ################################################################
  latent_dim: [7, 256] #!! 256 #2.[1, 512] # latent dimension
  latent_dim_stage1: 7 #!!
  motion_conditioning: last #last # last, full, middle(TODO)
  vae_from_t2m: False
  sample_latent_idx: 'single' # single, multi [MULTI DA IMPLEMENTARE I PADDING ANCORA!]
  FORCE_SEQ: False
  ###############################################################

#####################################
# The following are loggers settings
#####################################
LOGGER:
  SACE_CHECKPOINT_EPOCH: 200
  LOG_EVERY_STEPS: 1
  VAL_EVERY_STEPS: 200
  TENSORBOARD: True
  WANDB:
    PROJECT: 'motion_synthesis'
    OFFLINE: False
    RESUME_ID: null
    ENTITY: 'pinlab-sapienza'
